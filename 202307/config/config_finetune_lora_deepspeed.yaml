model:
  # model parallel training が可能なモデルについては、device_map: auto とすることで自動的にモデルを分割して複数GPUに読み込んでくれる
  # deepspeed を用いる場合は、重みの配置を deepspeed に任せるので auto は設定しない
  # device_map: auto
  # 8bit/4bit training may be incompatible with V100.
  # load_in_8bit: true
  trust_remote_code: true
  torch_dtype: torch.float16

lora:
  r: 4
  lora_alpha: 2
  target_modules:
    # for GPT-2 (includes cerebras/Cerebras-GPT-xx)
    - c_attn
    # for GPT-NEOX (includes databricks/dolly-v2-xx, cyberagent/open-calm-xx)
    # - query_key_value
    # - dense
    # - dense_h_to_4h
    # - dense_4h_to_h
    # for mosicml/mpt-7b
    # - Wqkv
  lora_dropout: 0.01
  bias: none
  task_type: CAUSAL_LM
  # set true for GPT-2
  # fan_in_fan_out: true
  fan_in_fan_out: false

training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  num_train_epochs: 3
  fp16: false
  optim: "adamw_torch"
  learning_rate: 1.0e-3
  logging_steps: 10
  evaluation_strategy: "steps"
  save_strategy: "steps"
  eval_steps: 500
  save_steps: 100
  save_total_limit: 3
  deepspeed: ds_config/ds_config_zero3.json
  # for mosicml/mpt-7b
  # deepspeed: ds_config/ds_config_zero3_mpt3.json

data:
  train_file: data/jsquad-valid-v1.1.json
  valid_size: 0.1

input_template: |-
  タイトル: {title}
  
  文脈: {context}

  設問: {question}

  答え: {answer[text]}

outputs:
  dirname: /scratch/${USER}/models/${JOB_ID}
 
