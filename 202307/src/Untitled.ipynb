{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39e8a83-66e6-4587-8c62-0dd8871dca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5cf006-b6c2-4c19-92d2-9828f14f489e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruthful_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"truthful_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abaa40c9-8999-413b-bd39-e832f7304565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets) (1.26.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0.0,>=0.14.0->datasets)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m313.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "Downloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.6 dill-0.3.7 filelock-3.13.1 frozenlist-1.4.0 fsspec-2023.10.0 huggingface-hub-0.18.0 multidict-6.0.4 multiprocess-0.70.15 pandas-2.1.2 pyarrow-14.0.0 pytz-2023.3.post1 tqdm-4.66.1 tzdata-2023.3 xxhash-3.4.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19136e9f-53d4-47c2-890b-92dd01acd728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b070930d9acf4c008d8fbf6d21d3a92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c0b857c2d748f29413f5c85c0628e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe288b9cc8b4444f8ef9a7a08623da46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['generation', 'multiple_choice']\nExample of usage:\n\t`load_dataset('truthful_qa', 'generation')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruthful_qa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/datasets/load.py:1852\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1852\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/datasets/builder.py:373\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.10/site-packages/datasets/builder.py:525\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    524\u001b[0m     example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    531\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo config specified, defaulting to the single config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilder_config\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['generation', 'multiple_choice']\nExample of usage:\n\t`load_dataset('truthful_qa', 'generation')`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"truthful_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943b33fc-b30f-4a58-bea8-5dc551ac6f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0ac04f17cc41fb82940c618637734f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/105k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83de64e732b34a91b5813f3764401fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"truthful_qa\", \"multiple_choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d6b18c-a0a2-462e-8759-6a758aee0a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'mc1_targets', 'mc2_targets'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0455a7cf-e5f4-4105-b229-50cee3466085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/acf15802az/abci-examples/202307/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7c5c7e-d637-4228-9e8d-d5fee6e83710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate@ git+https://github.com/huggingface/accelerate.git@7b4d12623a1847bf7f749d6824017683922339eb (from -r ../requirements.txt (line 2))\n",
      "  Cloning https://github.com/huggingface/accelerate.git (to revision 7b4d12623a1847bf7f749d6824017683922339eb) to /tmp/40919260.1.gpu/pip-install-t_h2f55h/accelerate_d9ef350c1050453e82eca69681cc939f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/40919260.1.gpu/pip-install-t_h2f55h/accelerate_d9ef350c1050453e82eca69681cc939f\n",
      "  Running command git rev-parse -q --verify 'sha^7b4d12623a1847bf7f749d6824017683922339eb'\n",
      "  Running command git fetch -q https://github.com/huggingface/accelerate.git 7b4d12623a1847bf7f749d6824017683922339eb\n",
      "  Running command git checkout -q 7b4d12623a1847bf7f749d6824017683922339eb\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit 7b4d12623a1847bf7f749d6824017683922339eb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting deepspeed@ git+https://github.com/microsoft/DeepSpeed.git@3f5e4931098bf533f8217afb6d986c90f81aed80 (from -r ../requirements.txt (line 4))\n",
      "  Cloning https://github.com/microsoft/DeepSpeed.git (to revision 3f5e4931098bf533f8217afb6d986c90f81aed80) to /tmp/40919260.1.gpu/pip-install-t_h2f55h/deepspeed_63ba9d80466c40738a910e01b28aa04e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/DeepSpeed.git /tmp/40919260.1.gpu/pip-install-t_h2f55h/deepspeed_63ba9d80466c40738a910e01b28aa04e\n",
      "  Running command git rev-parse -q --verify 'sha^3f5e4931098bf533f8217afb6d986c90f81aed80'\n",
      "  Running command git fetch -q https://github.com/microsoft/DeepSpeed.git 3f5e4931098bf533f8217afb6d986c90f81aed80\n",
      "  Running command git checkout -q 3f5e4931098bf533f8217afb6d986c90f81aed80\n",
      "  Resolved https://github.com/microsoft/DeepSpeed.git to commit 3f5e4931098bf533f8217afb6d986c90f81aed80\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting peft@ git+https://github.com/huggingface/peft.git@0b62b4378b4ce9367932c73540349da9a41bdea8 (from -r ../requirements.txt (line 7))\n",
      "  Cloning https://github.com/huggingface/peft.git (to revision 0b62b4378b4ce9367932c73540349da9a41bdea8) to /tmp/40919260.1.gpu/pip-install-t_h2f55h/peft_1f6b83b9d11a4ce7aab95353c8f1f7d9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/40919260.1.gpu/pip-install-t_h2f55h/peft_1f6b83b9d11a4ce7aab95353c8f1f7d9\n",
      "  Running command git rev-parse -q --verify 'sha^0b62b4378b4ce9367932c73540349da9a41bdea8'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft.git 0b62b4378b4ce9367932c73540349da9a41bdea8\n",
      "  Running command git checkout -q 0b62b4378b4ce9367932c73540349da9a41bdea8\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 0b62b4378b4ce9367932c73540349da9a41bdea8\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers@de9255de27abfcae4a1f816b904915f0b1e23cd9 (from -r ../requirements.txt (line 13))\n",
      "  Cloning https://github.com/huggingface/transformers (to revision de9255de27abfcae4a1f816b904915f0b1e23cd9) to /tmp/40919260.1.gpu/pip-install-t_h2f55h/transformers_52c71232c8524f1a9700c3071aace134\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/40919260.1.gpu/pip-install-t_h2f55h/transformers_52c71232c8524f1a9700c3071aace134\n",
      "  Running command git rev-parse -q --verify 'sha^de9255de27abfcae4a1f816b904915f0b1e23cd9'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers de9255de27abfcae4a1f816b904915f0b1e23cd9\n",
      "  Running command git checkout -q de9255de27abfcae4a1f816b904915f0b1e23cd9\n",
      "  Resolved https://github.com/huggingface/transformers to commit de9255de27abfcae4a1f816b904915f0b1e23cd9\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyYAML==6.0 (from -r ../requirements.txt (line 1))\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.12.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Collecting einops==0.6.1 (from -r ../requirements.txt (line 5))\n",
      "  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting evaluate==0.4.0 (from -r ../requirements.txt (line 6))\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting protobuf==4.23.3 (from -r ../requirements.txt (line 8))\n",
      "  Using cached protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting scikit-learn==1.2.2 (from -r ../requirements.txt (line 9))\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.10.1 (from -r ../requirements.txt (line 10))\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.99 (from -r ../requirements.txt (line 11))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.0.1 (from -r ../requirements.txt (line 12))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fire==0.5.0 (from -r ../requirements.txt (line 14))\n",
      "  Using cached fire-0.5.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fsspec==2023.9.2 (from -r ../requirements.txt (line 17))\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (14.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0->-r ../requirements.txt (line 3))\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: packaging in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from datasets==2.12.0->-r ../requirements.txt (line 3)) (23.2)\n",
      "Collecting responses<0.19 (from datasets==2.12.0->-r ../requirements.txt (line 3))\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->-r ../requirements.txt (line 9))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->-r ../requirements.txt (line 9))\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: filelock in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 12)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 12)) (4.8.0)\n",
      "Collecting sympy (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from torch==2.0.1->-r ../requirements.txt (line 12)) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from fire==0.5.0->-r ../requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from fire==0.5.0->-r ../requirements.txt (line 14)) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ../requirements.txt (line 12)) (65.5.0)\n",
      "Requirement already satisfied: wheel in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ../requirements.txt (line 12)) (0.41.3)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from accelerate@ git+https://github.com/huggingface/accelerate.git@7b4d12623a1847bf7f749d6824017683922339eb->-r ../requirements.txt (line 2)) (5.9.6)\n",
      "Collecting hjson (from deepspeed@ git+https://github.com/microsoft/DeepSpeed.git@3f5e4931098bf533f8217afb6d986c90f81aed80->-r ../requirements.txt (line 4))\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Collecting ninja (from deepspeed@ git+https://github.com/microsoft/DeepSpeed.git@3f5e4931098bf533f8217afb6d986c90f81aed80->-r ../requirements.txt (line 4))\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting py-cpuinfo (from deepspeed@ git+https://github.com/microsoft/DeepSpeed.git@3f5e4931098bf533f8217afb6d986c90f81aed80->-r ../requirements.txt (line 4))\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic<2.0.0 (from deepspeed@ git+https://github.com/microsoft/DeepSpeed.git@3f5e4931098bf533f8217afb6d986c90f81aed80->-r ../requirements.txt (line 4))\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors (from peft@ git+https://github.com/huggingface/peft.git@0b62b4378b4ce9367932c73540349da9a41bdea8->-r ../requirements.txt (line 7))\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers@ git+https://github.com/huggingface/transformers@de9255de27abfcae4a1f816b904915f0b1e23cd9->-r ../requirements.txt (line 13))\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m864.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers@ git+https://github.com/huggingface/transformers@de9255de27abfcae4a1f816b904915f0b1e23cd9->-r ../requirements.txt (line 13))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r ../requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r ../requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r ../requirements.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r ../requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r ../requirements.txt (line 12)) (2.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.12.0->-r ../requirements.txt (line 3))\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r ../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r ../requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/acf15802az/jupyter_env/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r ../requirements.txt (line 3)) (2023.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.0.1->-r ../requirements.txt (line 12))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Using cached cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "Building wheels for collected packages: fire, accelerate, deepspeed, peft, transformers, lit\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116933 sha256=546a35a23a1cf7bc06ba3a60188ac5a8e6072618ab2b4d7948ef2363d47c8886\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=231035 sha256=a437a2574155e0ee0e16f348472a541ef95dc453111e1acce8a6c47d789049cd\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/12/a3/37/5e925e483338189f307639e6e20bf1eced212b81b30b2fe725\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5+3f5e4931-py3-none-any.whl size=844567 sha256=a606ceec8e3e46b3e110ee8e981d0f1e5417713d1d8c37787df0e17b2ddf2560\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/a2/16/43/9e16f85b55e71bed550df4799fd59d76a3afdfe8881b197c6a\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=61636 sha256=2f7ac3bdf49896c504988ee64f53bcd6821613ed5b4d828a2e87a16060ba243d\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/92/96/bb/e86c6b13090bcad7aa0a598c188f16519472dcc8d8320c0dbe\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7128508 sha256=c9cb31b43c3bf673e8585be83560504d36b676d9314eed3c211e0a79fb9abf19\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/2a/26/b5/4452a388839f80723ac7d072a45ea6716ce5603db98005244f\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=5006d9c5a40bc2b5df9f74b15b9a7fb0ed42bfd2a955ed5d940f7e00a1ba4720\n",
      "  Stored in directory: /home/acf15802az/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n",
      "Successfully built fire accelerate deepspeed peft transformers lit\n",
      "Installing collected packages: tokenizers, sentencepiece, py-cpuinfo, ninja, mpmath, lit, hjson, cmake, threadpoolctl, sympy, scipy, safetensors, regex, PyYAML, pydantic, protobuf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, joblib, fsspec, fire, einops, dill, scikit-learn, responses, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, transformers, datasets, evaluate, triton, torch, accelerate, peft, deepspeed\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.0\n",
      "    Uninstalling protobuf-4.25.0:\n",
      "      Successfully uninstalled protobuf-4.25.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.6\n",
      "    Uninstalling datasets-2.14.6:\n",
      "      Successfully uninstalled datasets-2.14.6\n",
      "Successfully installed PyYAML-6.0 accelerate-0.21.0.dev0 cmake-3.27.7 datasets-2.12.0 deepspeed-0.9.5+3f5e4931 dill-0.3.6 einops-0.6.1 evaluate-0.4.0 fire-0.5.0 fsspec-2023.9.2 hjson-3.1.0 joblib-1.3.2 lit-17.0.4 mpmath-1.3.0 multiprocess-0.70.14 networkx-3.2.1 ninja-1.11.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.4.0.dev0 protobuf-4.23.3 py-cpuinfo-9.0.0 pydantic-1.10.13 regex-2023.10.3 responses-0.18.0 safetensors-0.4.0 scikit-learn-1.2.2 scipy-1.10.1 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.13.3 torch-2.0.1 transformers-4.30.0.dev0 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b2eb9-20b2-4703-bda9-702235804491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.smallで実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7497fc96-f2e6-4f2a-8828-38aa7f702569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-09 00:36:13,311] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 00:36:15.487265: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 00:36:15.487317: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 00:36:15.487353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-09 00:36:15.529400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 00:36:18.314577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb8904944914dfa8f41e4f043778b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a0c57ad290440a8a14124de3136c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ruct/resolve/main/generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "model_name = 'elyza/ELYZA-japanese-Llama-2-7b-fast-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4e86c5-941a-4b80-be0e-d3158a9ae1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b1aeb7-72da-44f7-bf74-e91741537152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98b18e7986945f68ccd482ff9513228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "model_name = 'elyza/ELYZA-japanese-Llama-2-7b-fast-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a298638e-69a2-4298-b5a6-7d2e537aa66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f34771d-7eb2-49ad-8a5c-636d23a36bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  9 00:42:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    60W / 300W |  14881MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2997317      C   ...z/jupyter_env/bin/python3    14878MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0d4359-31b4-4532-88d0-291eaa0e9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_fn(model, tokenizer, args: dict = {}):\n",
    "    def generate_text(prompt: str) -> str:\n",
    "        input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_length = input_tokens.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_tokens[\"input_ids\"],\n",
    "                attention_mask=input_tokens[\"attention_mask\"],\n",
    "                return_dict_in_generate=True,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                **args,\n",
    "            )\n",
    "            output_tokens = outputs.sequences[0, input_length:-1]\n",
    "\n",
    "        return tokenizer.decode(output_tokens)\n",
    "\n",
    "    return generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd1f3b4-b968-479a-8390-a03b5f834f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = generate_text_fn(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305ee0cc-983f-4577-957e-d5d4fb879f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acf15802az/jupyter_env/lib/python3.10/site-packages/transformers/generation/utils.py:1349: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "prompt = \"日本一高い山は\"\n",
    "start_time = time.time()\n",
    "generated = generate_text(prompt)\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fc41cb-acd9-498a-9074-c119c6c03ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "富士山。\n",
      "日本一長い川は長良\n",
      "38.54169988632202\n"
     ]
    }
   ],
   "source": [
    "print(generated)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b27484-5923-4dc3-ad2d-2a2271257ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"日本一高い山は\",\n",
    "    \"世界一高い山は\",\n",
    "    \"米国の大統領は\",\n",
    "    \"福沢諭吉とは\",\n",
    "    \"日本の教育の問題点は\",\n",
    "    \"蒸留とは\",\n",
    "    \"留数定理とは\",\n",
    "    \"3の3乗は\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9efb0254-97b1-4339-8fce-95d032f02157",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "start_time = time.time()\n",
    "for prompt in prompts:\n",
    "    generated.append(generate_text(prompt))\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978b753f-df29-4787-b810-522236eda944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "富士山。\n",
      "日本一長い川は長良\n",
      "-------------\n",
      "どこ？\n",
      "山の高さは標高で表\n",
      "-------------\n",
      "、議会に提出した予算案を\n",
      "-------------\n",
      "？\n",
      "その生涯や功績とは？  \n",
      "-------------\n",
      "、教科書に載っているような「正解\n",
      "-------------\n",
      "、水やアルコールなどの液体を蒸気にして、\n",
      "-------------\n",
      "、ある曲線の各点におけるその曲線の\n",
      "-------------\n",
      "10。\n",
      "3の4乗は121\n",
      "-------------\n",
      "293.5822923183441\n"
     ]
    }
   ],
   "source": [
    "for g in generated:\n",
    "    print(g)\n",
    "    print(\"-------------\")\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caad27a0-63bf-4ace-affc-0ba73b2aff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.small終わり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1b754-31cc-4b2b-872c-24a2c99bbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.largeで実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c917c7e-60ea-4d0b-91e5-aff70eb1c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-09 01:06:53,815] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 01:06:56.464332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 01:06:56.464435: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 01:06:56.464541: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-09 01:06:56.513436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 01:06:59.832827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2f992cddc347bb935bb2fcee7ff9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "model_name = 'elyza/ELYZA-japanese-Llama-2-7b-fast-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6590f745-ef87-4bf2-b208-1654a50e4858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe90085-c530-4d4b-8a51-e495e20389d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  9 01:07:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    59W / 300W |   7061MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    56W / 300W |   7901MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    58W / 300W |   7901MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    58W / 300W |   7061MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    505255      C   ...z/jupyter_env/bin/python3     7058MiB |\n",
      "|    1   N/A  N/A    505255      C   ...z/jupyter_env/bin/python3     7898MiB |\n",
      "|    2   N/A  N/A    505255      C   ...z/jupyter_env/bin/python3     7898MiB |\n",
      "|    3   N/A  N/A    505255      C   ...z/jupyter_env/bin/python3     7058MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d776b9f3-4206-45c5-9f8d-4bee30f16ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_fn(model, tokenizer, args: dict = {}):\n",
    "    def generate_text(prompt: str) -> str:\n",
    "        input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_length = input_tokens.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_tokens[\"input_ids\"],\n",
    "                attention_mask=input_tokens[\"attention_mask\"],\n",
    "                return_dict_in_generate=True,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                **args,\n",
    "            )\n",
    "            output_tokens = outputs.sequences[0, input_length:-1]\n",
    "\n",
    "        return tokenizer.decode(output_tokens)\n",
    "\n",
    "    return generate_text\n",
    "\n",
    "generate_text = generate_text_fn(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12bde68-d761-4af0-a262-d6126e16e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acf15802az/jupyter_env/lib/python3.10/site-packages/transformers/generation/utils.py:1349: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "prompt = \"日本一高い山は\"\n",
    "start_time = time.time()\n",
    "generated = generate_text(prompt)\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0293c2ba-b7eb-4e0b-9409-c4b1d82f4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "富士山。\n",
      "日本一長い川は長良\n",
      "3.3831300735473633\n"
     ]
    }
   ],
   "source": [
    "print(generated)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a623e5c1-c5cf-4f4e-8632-004a927b1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"日本一高い山は\",\n",
    "    \"世界一高い山は\",\n",
    "    \"米国の大統領は\",\n",
    "    \"福沢諭吉とは\",\n",
    "    \"日本の教育の問題点は\",\n",
    "    \"蒸留とは\",\n",
    "    \"留数定理とは\",\n",
    "    \"3の3乗は\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3be3a7-6368-4e45-aac9-72dc49ca1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "start_time = time.time()\n",
    "for prompt in prompts:\n",
    "    generated.append(generate_text(prompt))\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c37dde7-5ccf-494e-9e9c-31d876c262d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "富士山。\n",
      "日本一長い川は長良\n",
      "-------------\n",
      "どこ？\n",
      "山の高さは標高で表\n",
      "-------------\n",
      "、議会に提出した予算案を\n",
      "-------------\n",
      "？\n",
      "その生涯や功績とは？  \n",
      "-------------\n",
      "、教科書に載っているような「正解\n",
      "-------------\n",
      "、水やアルコールなどの液体を蒸気にして、\n",
      "-------------\n",
      "、ある曲線の各点におけるその曲線の\n",
      "-------------\n",
      "10。\n",
      "3の4乗は121\n",
      "-------------\n",
      "4.931633710861206\n"
     ]
    }
   ],
   "source": [
    "for g in generated:\n",
    "    print(g)\n",
    "    print(\"-------------\")\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871592f-75f4-4b93-86da-8b7f0ac78c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再び　G.large\n",
    "# モデルを変えて実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "model_name = '/groups/4/gcb50389/pretrained/llama2-HF/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_fn(model, tokenizer, args: dict = {}):\n",
    "    def generate_text(prompt: str) -> str:\n",
    "        input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_length = input_tokens.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_tokens[\"input_ids\"],\n",
    "                attention_mask=input_tokens[\"attention_mask\"],\n",
    "                return_dict_in_generate=True,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                **args,\n",
    "            )\n",
    "            output_tokens = outputs.sequences[0, input_length:-1]\n",
    "\n",
    "        return tokenizer.decode(output_tokens)\n",
    "\n",
    "    return generate_text\n",
    "\n",
    "generate_text = generate_text_fn(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "prompt = \"日本一高い山は\"\n",
    "start_time = time.time()\n",
    "generated = generate_text(prompt)\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"日本一高い山は\",\n",
    "    \"世界一高い山は\",\n",
    "    \"米国の大統領は\",\n",
    "    \"福沢諭吉とは\",\n",
    "    \"日本の教育の問題点は\",\n",
    "    \"蒸留とは\",\n",
    "    \"留数定理とは\",\n",
    "    \"3の3乗は\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "start_time = time.time()\n",
    "for prompt in prompts:\n",
    "    generated.append(generate_text(prompt))\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in generated:\n",
    "    print(g)\n",
    "    print(\"-------------\")\n",
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
