INFO:root:model_name: /groups/4/gcb50389/pretrained/llama2-HF/Llama-2-7b-hf
INFO:root:load tokenizer
Using pad_token, but it is not set yet.
INFO:root:set pad_token to </s>
INFO:root:load model: {'pretrained_model_name_or_path': '/groups/4/gcb50389/pretrained/llama2-HF/Llama-2-7b-hf', 'device_map': 'auto', 'trust_remote_code': True}
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/40877670.1.gpu/tmpydp5kntz
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/40877670.1.gpu/tmpydp5kntz/_remote_module_non_scriptable.py
[2023-11-01 22:22:57,166] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [10:38<10:38, 638.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [12:24<00:00, 325.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [12:24<00:00, 372.34s/it]
INFO:root:processing R1 data.
WARNING:datasets.builder:Found cached dataset json (/scratch/acf15802az/.cache/huggingface/datasets/json/default-9daf52417ac5f3f8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
INFO:root:processing 20th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
INFO:root:processing 21th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
INFO:root:processing R2 data.
WARNING:datasets.builder:Found cached dataset json (/scratch/acf15802az/.cache/huggingface/datasets/json/default-e81a17a045232274/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
INFO:root:processing 20th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
INFO:root:processing 21th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
INFO:root:processing R3 data.
WARNING:datasets.builder:Found cached dataset json (/scratch/acf15802az/.cache/huggingface/datasets/json/default-5930e0f5dc0720f3/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
INFO:root:processing 20th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
INFO:root:processing 21th data.
Both `max_new_tokens` (=16) and `max_length`(=4096) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
